---
layout: post
title:  "《这就是搜索引擎 核心技术详解》读书笔记"
date:   2015-03-24 12:00:00
tags:	tech
myTag:	algorithm

---

# 《这就是搜索引擎 核心技术详解》读书笔记

---------------------------------------------------

### 第二章 网络爬虫

### 2.4 抓取策略

在爬虫系统中，待抓取URL队列是很关键的部分，需要爬虫爬取的网页URL在其中顺序排列，形成一个队列结构，调度程序每次从队列头取出某个URL，发送给网页下载器下载页面内容，每个新下载的页面包含的URL会追加到待抓取URL队列的末尾，形成循环，整个爬虫系统由这个队列驱动运转的。

爬虫的不同爬取策略，是利用不同的方式确定待抓取URL队列中URL的优先顺序。抓取策略中，比较有代表性的解决方案有4种: 宽度优先便利策略、非完全PageRank策略、OCIP策略以及大站优先策略。

#### 2.4.1 宽度优先遍历策略 (Breath First)

宽度优先遍历策略是一种简单直观而且历史悠久的遍历方法，在搜索引擎爬虫一出现就开始采用，新提出的爬取策略往往会将这种方法作为比较基准。值得注意的是，很多新的方法实际效果不见得比宽度优先遍历策略好，所以至今这种方法也是很多实际爬虫系统优先采用的爬取策略。

宽度优先策略就是`"将新下载网页包含的链接直接追加到待抓取的URL队列末尾"`。

实验表明这种策略效果很好，虽然看似机械，但实际上的网页抓取顺序基本是按照网页的重要性排序的。之所以如此，有研究人员认为：如果某个网页包含很多入链，那么更有可能被宽度优先遍历策略早早抓到，而入链个数从侧面体现了网页的重要性，即实际上宽度优先遍历策略隐含了一些网页优先级假设。

#### 2.4.2 非完全PageRank策略 (Partial PageRank)

PageRank 可以用来衡量网页的重要性，可以想到用PageRank的思想来对URL优先级进行排序。但是PageRank是一个全局性算法，也就是当所有网页都下载完成后，其计算结果才是可靠的，而爬虫在爬取页面的过程中只能看到一部分页面，所以在抓取阶段的页面是无法得到可靠PageRank得分的。

但是我们可以在这个不完整的网页子集内计算PageRank。这就是非完全PageRank策略的基本思路: `对已下载的网页，加上待抓取URL队列中的URL一起，形成网页集合，在此集合内进行PageRank计算，计算完成后，将待抓取URL队列里的页面按照PageRank得分由高到低排序，形成的序列就是爬虫接下来应该依次爬取的URL列表。`这就是"非完全PageRank"策略。

如果每抓取一个页面就计算一次非完全PageRank值，明显效率太低，在现实中不可行。一个折中的办法是: 每当新下载K个页面后，再进行非完全PageRank的计算。但是这样又引来一个新的问题: 在展开下一轮PageRank计算之前，从新下载的页面抽取出包含的链接，很有可能这些链接的重要性非常高，理应优先下载。`非完全PageRank赋予这些新抽取出来但是又没有PageRank值的网页一个临时PageRank值，将这个网页的所有入链传导的PageRank值汇总，作为临时的PageRank值，如果这个值比待抓取URL队列中以及计算出来的PageRank值的网页高，那么优先下载这个URL`。

非完全PageRank看上去相对复杂，不同的实验结果有些表明非完全PageRank结果略优，有些结果则恰恰相反。更有研究人员指出: 非完全PageRank计算得出的重要性和完整的PageRank计算结果差异很大，不应作为衡量抓取过程中URL重要性的依据。

#### 2.4.3 OCIP 策略 (Online Page Importance Computation)

OCIP可以看成是一种改进的PageRank算法。`在算法开始前，每个页面都给予相同的"现金"(cash)，每当下载某个页面P后，P将自己拥有的现金平均分配给页面中包含的链接页面，把自己的现金清空。而对于待抓取的URL队列中的页面，则根据其手头上拥有的现金金额多少排序，优先下载现金最充裕的页面。`OICP从大的框架和PageRank思路基本一直，区别在于: PageRank每次需要迭代计算，而OCIP策略不需要迭代过程，所以计算速度远远快语PageRank，适合实时计算使用。同时，PageRank在计算时，存在向无链接关系页面远程跳转的过程，而OCIP没有这一计算因子。实现结果证明，OCIP是中较好的重要性衡量策略，效果略优于宽度优先遍历策略。

#### 2.4.4 大站优先策略 (Larger Sites First)

大站优先策略思路很直接: `以网站为单位衡量网页重要性，对于待抓取URL队列中的页面，根据所属网站归类，如果那个网站等待下载的页面最多，则优先下载这些链接。`其本质思想倾向于优先下载大型网站，因为大型网站往往包含更多的页面，鉴于大型网站往往是著名企业的内容，其网页质量一般较高，所以这个思路简单，但有一定依据。实现表明这个算法效果也要略优于宽度优先遍历策略。

---------------------------------------------

### 第三章 搜索引擎索引

### 3.1 索引基础

#### 3.1.1 单词-文档矩阵

单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型。

	文档.	|	doc1	|	doc2	|	doc3	|	doc4	|	doc5
	--------------------------------------------------------------------
	词汇1	|	√		|			|			|			|	√
	词汇2	|			|	√		|	√		|			|	
	词汇3	|			|			|			|	√		|	
	词汇4	|	√		|			|			|			|	√
	词汇5	|			|	√		|			|			|	
	词汇6	|			|			|	√		|			|	

搜索引擎的索引就是实现单词-文档矩阵的数据结构。有很多不同方式实现上述概念模型，比如倒排索引、签名文件、后缀树等。各项实验数据表明，倒排索引是单词-文档映射关系的最佳实现方式。

#### 3.1.3 倒排索引简单实例

中文和英文不同，单词之间没有明确的分隔符号，所以首先要用分词系统将文档自动切分成单词序列，这样每个文档就转化为由单词序列构成的数据流。每个不同的单词赋予唯一的`单词编号`，同时记录下哪些文档包含这个单词。

在单词对应的倒排列表中不仅记录了文档编号，还记载了单词频率信息(TF)，即单词在某个文档中出现的次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相识度是一个很重要的计算因子，所以将其记录在倒排列表中，方便后续排序进行分值计算。

除了记录文档编号和单词频率信息外，实用的倒排索引还额外记载了两类信息，即每个单词对应的文档频率信息以及单词在某个文档出现的位置信息。文档频率信息代表了在文档集合中由多少个文档包含某个单词，这个信息在搜索结构排序计算中也是一个重要的因子。而单词在某个文档中出现位置的信息并非索引系统一定要记录的，之所以说这个信息对搜索系统来说并非必要，是因为位置信息只有在支持短语查询的时候才能够派上用场。

-----------------------------------

### 3.2 单词词典

单词词典是用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里面查询，能够获得相应的倒排列表。对于一个规模很大的文档集合来说，可能包含百万个不同单词，能够快速定位某个单词，直接影响搜索速度，所有需要高效的数据结构对单词词典进行构造和查找，常用的数据结构包括哈希加链表结构和树形词典结构。

#### 3.2.1 哈希加链表

主体是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在通途链表里，相同哈希值的单词形成链表结构。

在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档时，对于某个文档中出现的单词T，首先利用哈希函数获得哈希值，之后根据哈希值读取对应哈希表项其中保存的指针，找到对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解释的文档里已经出现过。如果冲突链表里没有这个单词，说明该单词是首次出现，则加入冲突链表。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就简历起来了。

#### 3.2.2 树形结构

B树(或者B+树)是另一种高效查找结构。B树与哈希查找方式不同，需要字典能够按照大小排序，而哈希方式无需数据满足此项要求。

B树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到了根据词典项比较大小进行导航的作用。最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。

-----------------------------------

### 3.3 倒排列表(Posting List)

	+-------+	+---------------------------+---------------------------+---------------------------+
	| word	|-->| [DocID,TF,<pos1,pos2,...>]| [DocID,TF,<pos1,pos2,...>]| [DocID,TF,<pos1,pos2,...>]|
	+-------+	+---------------------------+---------------------------+---------------------------+
						Posting Item				Posting Item				Posting Item
							|															|			
							+-----------------------------+-----------------------------+
													Posting List
	Dictionary Item

上图是之前介绍的简单索引例子。在实际的搜索引擎系统中，并不存储倒排索引项中的实际文档编号，而是取而代以文档编号差值(D-Gap)。文档编号差值是倒排列表中相邻的两个倒排索引项文档编号的差值，一般在索引构建过程中，可以保证倒排列中后面出现的文档编号大于之前出现的文档编号，所以文档编号差值总是大于0的整数。

之所以要对文档编号进行差值计算，主要原因是为了更好地对数据进行压缩，原始文档编号一般都是大数值，通过差值计算，就有效地将大数值转换为了小数值，而有助于增加数据地压缩率。第四章将详细讲述索引压缩及其原因。

------------------------------------

### 3.4 建立索引

#### 3.4.1 两遍文档遍历法 (2-Pass In-Memory Inversion)

**第一次文档遍历**

第一次扫描文档集合时，该方法并没有立即开始建立索引，而是收集全局统计信息。比如文档集合包含的文档个数N，文档集合内所包含的不同单词个数M，每个单词在多少个文档中出现过的信息DF。将所有单词对应的DF值全部相加，就可以知道建立最终索引所需的内存大小是多少，因为一个单词对应的DF值如果是10，说明有10个文档包含这个单词，那么这个单词对应的倒排列表应该包含10项内容，每一项计算某个文档的文档ID和单词在该文档对应的出现次数TF。

在获得上述3类信息后，就可以知道最终索引的大小，于是在内存中分配足够大的空间，用来存储倒排索引内容。在内存中可以开辟连续存储区域，因为第一遍扫描已经获得每个单词的DF信息，所以将连续存储区划分成不同的片段，词典内某个单词根据自己对应的DF信息，可以通过指针，指向属于自己的内存片段的起始位置和终止位置，将来在第二次扫描时，这个单词对应的倒排列表信息就会被填充进这个片段中。

综上所述，第一遍扫描的主要目的是获得一些统计信息，根据统计信息分配内存资源，建立好单词对应倒排列表在内存中的位置信息。

**第二遍文档遍历**

第二次扫描的时候，开始真正建立每个单词的倒排列表信息，对每个单词而言，获得包含这个单词的每个文档的文档ID，以及这个单词在文档中的出现次数TF，这样开始不断填充第一遍扫描所分配所指向的内存空间。当第二遍扫描结束的时候，分配的内存空间正好被填充满，而每个单词用指针指向的内存区域其起始位置和终止位置之间的数据就是这个单词对应的倒排列表。

所以的构建完全是在内存中完成的，这就要求内存一定要大，否则如果文档集合太大，内存未必能够满足需求。在建立索引的过程中，从磁盘读取并解析文档基本是最耗时的步骤，而两遍文档遍历法需要进行两遍遍历，所以速度上不占优，在实际中采用这种方法的系统并不常见。

#### 3.4.2 排序法 (Sort-based Inversion)

**中间结果内存排序**

两次遍历法对内存消耗要求高，当文档集合非常大时，可能因为内存不够，导致无法建立索引。排序法对次做了改进，该方法在建立索引的过程中，始终在内存中分配固定大小的空间，用来存放词典信息和索引的中间结果，当分配空间被消耗光的时候，把中间结果写入磁盘，清空内存中间结果所占空间，以用做下一轮存放索引中间结果的存储区。

读入文档后，对文档进行编号，赋予唯一的文档ID，并对文档进行内容解析。对于文档中出现的单词，通过查词典将单词转换为对应的单词ID，如果词典中没有这个单词，则赋予单词唯一的单词ID并插入词典。在完成类由单词映射为单词ID的过程之后，可以对该文档内每个单词建立一个(单词ID，文档ID，单词频率)三元组，这个三元组就是单词对应文档的倒排列表项，将这个三元组追加进中间结构存储区末尾。

随着新文档不断被完成，存储三元组的中间结果所占内存会越来越大，词典所包含新单词也越来越多，当分配的内存定额被占满时，对三元组中间结构进行排序。排序原则是: 主键是单词ID，首先按单词ID由小到大排序；次键是文档ID，即相同单词ID的情况下，按照文档ID从小到大排序。通过这个方法，三元组变成有序形式。为了腾出内存空间，将排好序的三元组写入磁盘临时文件中。需要注意的是: `在建立索引的过程中，词典是一直存储在内存中的，每次清空的内存只是将中间结果写入磁盘。`随着处理文档的增加，词典占用的内存会增多，由于分配内存是固定大小，而词典占用内存越来越大，即越往后，可以用来存储三元组的空间越来越少。

**合并中间结果**

因为每一轮处理都会在磁盘上产生一个对应的中间结果文件，在合并中间结果的过程中，系统为每个中间结果文件在内存中开辟一个数据缓冲区，用来存放文件的部分数据。因为在形成中间结果文件前，已经按照单词ID和文档ID进行了排序，所以进入缓冲区的数据已经是有序的。合并过程中，将不同缓冲区中包含同一个单词ID的三元组进行合并，如果某个单词ID的所有三元组全部合并完成，说明这个单词的倒排列表已经构建完成，则将其写入最终索引中，同时将各个缓冲区中对应的单词ID的三元组清空，当所有中间结果文件都依次被读入缓冲区，合并完成后，就形成了最终的索引文件。

#### 3.4.3 归并法 (Merge-based Inversion)

排序法只是将中间结果写入磁盘，而词典信息一直在内存中进行维护，随着处理文档越来越多，词典里包含的词典项占用越来越多内存，导致后期中间结果可用内存越来越少。归并法对此做了改进，即每次将内存中数据写入磁盘时，包括词典在内的所有中间信息都被写入磁盘，这样内存所有内容就可以被清空，后续建立索引可以使用全部的定额内存。

排序法在内存中存放的是词典信息和三元组数据，在建立过程中，词典和三元组数据并没有直接的联系，词典只是为了将单词映射为单词ID。而在归并法中则是在内存中建立一个完整的内存索引结构，相当于对目前处理的文档子集单独在内存中建立起一整套倒排索引，和最终索引相比，其结构和形式是相同的，区别只是这个索引只是部分文档的索引而非全部文档的索引。

其次，在将中间结果写入磁盘临时文件时，归并法将整个内存的倒排索引写入临时文件，对某个单词的倒排列表写入磁盘文件时，将词典项放在列表最前面，之后跟随相应的倒排列表，这样依次将单词和对应的倒排列表写入磁盘文件，随后彻底清空所占内存。

在最后的临时文件合并为最终索引的过程中，排序法因为保存的是有序三元信息，所以合并时，是对同一个单词的三元组依次进行合并；而归并法的临时文件则是每个单词对应的部分倒排列表，所以合并时针对每个单词的倒排列表进行合并，形成这个单词的最终倒排列表。

---------------------------

### 3.5 动态索引

由于互联网的内容是动态的，所以新产生的页面、删除的页面应该进入这个索引中。动态索引可以分成三部分: 倒排索引、临时索引和已删除文档列表。

倒排索引就是初始文档建立的索引结构，一般单词词典存储在内存，对应的倒排列表存储在磁盘文件中。临时索引就是当有新文档进入系统时，实时解析文档并将其追加到临时索引结构中，它的词典和倒排列表都存储在内存中。已删除文档列表就是记录已经被删除的文档的ID。需要注意的是一篇文档内容被更改了，可以认为是旧文档被删除，之后再向系统增加一篇新的文档，通过这种方式实现对内容更改的支持。

如果用户输入查询请求，搜索引擎同时从倒排索引和临时索引文件中读取用户查询单词的倒排列表，并对两个结果进行合并，之后利用删除文档列表进行过滤，最终形成搜索结果。

----------------------------

### 3.6 索引更新策略

由于临时索引文件也会使内存饱和，所以需要考虑索引的更新策略。常用的索引更新策略有4种: 完全重建策略、再合并策略、原地更新策略以及混合策略。

#### 3.6.1 完全重建策略(Complete Re-Build)

当新文档达到一定数量，将新增文档和原来的老文档进行合并，然后对所有文档重新建立索引。

因为重建索引需要较长时间，在进行索引重建的过程中，内存中依然维护老的索引，只有当新索引完全建立之后，才能释放旧的索引。

这种重建策略时候小文档集合，因为重建索引的代价较高，但是目前主流商业搜索引擎一般就是采用这种方式来维护索引的更新的。

#### 3.6.2 再合并策略 (Re-Merge)

当分配给临时索引文件的内存消耗完了，则把临时索引和老文档的倒排索引进行合并，以生成新的索引。

由于倒排索引和临时索引都是排好序的，所以只需要一次遍历两个索引就可以完成合并。

#### 3.6.3 原地更新策略 (In-Place)

原地更新策略希望如果老索引的倒排列表没有变化，可以不需要读取这些信息，只对那些倒排列表变化的单词进行处理。进一步希望: 即使老索引的倒排列表发生变化，是不是只再其末尾进行追加操作，而不用读取原来的倒排列表并写到磁盘另一个地方。

为了支持追加操作，原地更新策略在最初建立的索引中，就会在每个单词的倒排列表末尾预留出一定的磁盘空间。这样，在进行索引合并时，就可以将增量索引追加到预留空间。

原地更新策略的实验数据证明，其效率比再合并策略低，原因在于这个策略需要对磁盘可用空间进行维护和管理；另一方面由于破坏了单词的联系性，导致再索引合并时不能顺序读取，必须维护一个单词到其倒排文件对应位置的映射表，一方面降低磁盘读取速度，另一方面需要大量的内存来存储这类映射信息。

#### 3.6.4 混合策略 (Hybrid)

混合策略更具单词性质进行分类对其索引采取不同的更新策略。例如根据单词的倒排列表长度进行区分。常见单词，倒排列表长，就采用原地更新策略，而短倒排列表单词则采用再合并策略。

---------------------------------

### 3.7 查询处理

#### 3.7.1 一次一文档 (Doc at a Time)

搜索引擎收到查询后(例如输入“搜索引擎”和“技术”)，首先将两个单词的倒排列表从磁盘读入内存。例如文档1同时包含了这两个词，可以根据各自的TF和IDF等参数计算文档和查询单词的相似性，之后将两个分数相加获得文档1和用户查询的相似性得分。随后搜索系统处理文档2，文档2只包含“技术”不包含“搜索引擎”，同样计算相似性得分，依此类推。最终将最高的K个文档作为搜索结果输出。

实际上搜索引擎的输出结果往往是限定个数的，比如输出10个结果，所以实现一次一文档方式时，不必保存所有文档的相关性得分，只需要维护一个大小为K的优先队列，用来保存目前计算过程中最高的K个文档即可。

#### 3.7.2 一次一单词 (Term at a Time)

一次一单词的计算过程相当于先计算一个单词所有倒排列表文档的分数，再累加到下一个单词的倒排列表文档分数中。

每次计算完一个文档的相似性分数后存放进一个哈希表中，用作下一个词累加之用。

#### 3.7.3 跳跃指针 (Skip Pointers)

输入包含多个词的查询实际上是求包含这多个词的倒排列表中文档的交集。时间复杂度为O(m+n)。如果文档以文档编号值(D-Gap)形式记录，并且以压缩后的方式编码，那么求倒排索引交集就会复杂化，但是如果将倒排索引分块，并加入提示的指针，则可以加快求交集这一过程。

	+-----------+	+-------+-------+-------+-------+-------+-------+
	| Google	|-->| <5,1>	| <7,1>	| <12,2>| <13,3>| <15,2>| <19,1>|
	+-----------+	+-------+-------+-------+-------+-------+-------+
										|
										V	D-Gap
	+-----------+	+-------+-------+-------+-------+-------+-------+
	| Google	|-->| <5,1>	| <2,1>	| <5,2> | <1,3> | <2,2> | <4,1> |
	+-----------+	+-------+-------+-------+-------+-------+-------+
										|
										V	分块
	+-----------+	+-----------------------+-----------------------+
	| Google	|-->| <5,1>	  <2,1>	  <5,2> | <1,3>   <2,2>   <4,1> |
	+-----------+	+-----------------------+-----------------------+
										|
										V	插入跳跃指针
	+-----------+	+-----------+-----------------------+-----------+----------------------+
	| Google	|-->| <5,Pos1>	| <5,1>	  <2,1>	  <5,2> | <13,Pos2>	|<1,3>   <2,2>   <4,1> |..
	+-----------+	+-----------+-----------------------+-----------+----------------------+
					\-----------/						\-----------/
						  \-----------------------------------/\-----------------------------/

我们通过跳跃指针来判断需不需要把中间分块的那一块进行解码反求文档编号。

如何设定数据块的大小对于效率有影响，数据块小，则跳跃指针向后进行跳跃的可能性越大，但是增加了指针比较操作的次数；数据块大，可以减少指针比较的次数，但是使用跳跃指针向后跳跃的可能性小。实践表明一个简单有效的启发式规则是: 假设倒排列表长度为L，则使用sqrt(L)作为块大小，则效果较好。

---------------------------------

### 3.8 多字段索引

一般的文档都包含多个字段，例如论文中包含“标题”、“作者”、“摘要”、“正文”、“参考文献”等几个字段。搜索引擎应该能够支持用户指定某个字段作为搜索范围，因为我们知道如果搜索关键词出现在标题中往往比只出现在正文更关键。而实现多字段索引有3中方式: 多索引方式、倒排列表方式和扩展列表方式。

#### 3.8.1 多索引方式

多索引方式针对每个不同的字段，分别建立一个索引(相当于将一个文档拆分成几个文档)。当用户没有指定特定字段时，搜索引擎堆所有字段都进行馋着并合并多个字段的相关性得分，对于多索引方式，而需要对多个索引进行读取，所以这种方式的效率比较低。

#### 3.8.2 倒排列表方式

在每一项倒排列表中每个文档索引项的末尾追加字段信息，例如用3个bit位来记录某单词是不是出现在“标题”“摘要”“正文”中。

#### 3.8.3 扩展列表方式 (Extent List)

扩展列表是实际中应用得比较多得支持多字段索引的方法。这个方法需要为每个字段建立一个列表。这个列表记录每个文档，单词位置的起始和结束位置。例如

							+-----------+-----------+-----------+-----------+
	"Title" Extent List:	| <1,(1,4)> | <2,(1,7)> | <3,(1,3)> | <4,(1,6)> |
							+-----------+-----------+-----------+-----------+

第一项代表文档1的Title位置为从第一个单词到第四个单词。当进行多索引搜索的时候，只需要到指定字段的扩展列表中查找对应文档该字段的单词覆盖范围即可。

----------------------------------------

### 3.9 短语查询

几个经常连在一起使用的单词很可能就构成了一个短语，但是“你懂的”和“懂你的”含义相差甚于，所以搜索引擎需要保留单词之间的顺序关系。常见的支持短语查询的技术包括: 位置信息索引、双词索引以及短语索引这3类。为了能够更有效的利用存储和计算资源，也可以三者结合使用。

#### 3.9.1 位置信息索引 (Position Index)

由于倒排列表往往存储3种信息: 文档ID、单词词频和单词位置。所以在搜索短语的时候，实际上我们可以通过查看连接的两个词在其分别的倒排列表文档索引项种，出现的位置是不是也是相连的。

#### 3.9.2 双词索引 (Nextword Index)

短语至少包含两个词语，但是统计数据表明，二词短语在短语种所占比例最大，所以如果能对双词短语提供快速查询，也能解决短语查询的问题。

双词索引的基本想法就是把一维的词典变成两维的，一个“首词”后面接一个“下词”，然后计算包含这个短语的倒排列表。

但是很明显，双词索引使得索引的数据量激增，所以实际中，只有非常常用的短语(例如“我的”)才使用这种方法，对于一般短语，可以使用位置信息等常规手段来达到目的。 

#### 3.9.3 短语索引 (Phrase Index)

最直观的做法就是把短语看作是一个词语进行索引。

但是它的一个缺点就是没有办法事前就得到所有短语。通常的做法是挖掘出热门短语，并为他们专门建立索引。

#### 3.9.4 混合方法

对于输入的短语，首先在短语索引中查找，如果没有则到双词索引中查找，如果没有则从常规索引中对短语进行处理。

-----------------------------

### 3.10 分布式索引

有**按文档划分**和**按单词划分**两种方法。其中“按文档划分”的**可扩展性**较强，因为新进入的文档不会影响到所有机器的索引，反观“按单词划分”则可能会。在**负载均衡**方面，由于某些词可能比较热门，所以“按单词划分”也不是特别好。在**容错性**方法，如果一台索引服务器发生故障，“按单词划分”的方案会使得服务无法继续，而“按文档划分”的方案可以继续工作，对用户来说，不会发生直接感受到机器故障的影响。

---------------------------

### 第四章 索引压缩 

### 4.1 词典压缩

为了快速响应查询，词典数据(非倒排列表)通常都会常驻内存中。词典的组织方式有两种: 哈希加链表和B树词典结构。在链表内部或者B树的叶子节点会存储单词相关信息(单词本身、文档频率信息DF、以及指向倒排列表的指针信息)。

		+-------+-------+-------+
		| word	|  DF	|pointer|
		| ----  |  --   |-------|
		+-------+-------+-------+	+-----------------------+
	   /| abc	|234821 |	 ---|->	|						|
	  O +-------+-------+-------+	+---------------+-------+
	 / \| efg	|132160 |	 ---|->	|				|
	O	+-------+-------+-------+	+---------------+---+
	 \ /| ...	| ....	|	 ---|->	|					|
	  O +-------+-------+-------+	+-------------------+-+
	   \| wxyz	| 5015	|	 ---|->	|					  |
	    +-------+-------+-------+	+---------------------+
		 
		 20 Byte  4 Byte  4 Byte

有些词因为比较长(eg:"中华人民共和国")所以我们在word中需要分配较大的空间以保证所有词都能分配到足够空间，但是有些词比较短(eg:"我")，所以这个空间会造成一定的浪费。

我们可以把所有这些次放在一个连续的空间里，使用指针指向这些词。

	+-------+-------+-------+
	|  DF	|pointer|  word |		+---------------------------+
	|  --   |-------|  addr	|		|..."abc""def"..."wxyz"...	|
	+-------+-------+-------+		+---------------------------+
	|234821 |		|	 ---|------------A	  A    A  A
	+-------+-------+-------+				  |    |  |
	|132160 |		|	 ---|-----------------+    |  |
	+-------+-------+-------+					   |  |
	| ....	|		|	 ---|----------------------+  |
	+-------+-------+-------+						  |
	| 5015	|		|	 ---|-------------------------+
	+-------+-------+-------+
		
	 4 Byte   4 Byte  4 Byte

此外还可以进一步进行压缩，将几个连续相连的单词公用一个单词地址指针，在连续单词中增加单词长度信息，使得减少单词地址指针的个数。

经过优化的词典比没有经过优化的词典节省内存60%。

---------------------------

### 4.2 倒排列表压缩算法

评判压缩算法的指标为: 压缩率、压缩速度和解压速度。

#### 4.2.2 一元编码和二进制编码

**一元编码**

		1:		0
		2:		10
		3:		110
		4:		1110
		5:		11110

一元编码比较适合数字小的情况。

#### 4.2.3 Elias Gamma算法与Elias Delta算法

对于一个数x，

	x = 2^e + d

我们可以把一个大数x拆成e和d两个因子。例如数字9，我们得到e=3,d=1。我们对e+1进行一元编码，对d采用宽度为e的二进制进行表示。于是9的Elias Gamma编码为1110:001。

Elias Delta则是对进行Elias Gamma后的e+1再进行一次Elias Gamma。对于数字9，它的因子e=3，所以我们对e+1进行Elias Gamma，4的Elias Gamma为110:00，所以9的Elias Delta为110:00:001。

对于大数字数据来说，Elias Delta的压缩效果更好。

#### 4.2.4 Golomb算法与Rice算法

Golomb算法和Rice算法的思路与Elias算法一样，即根据分解函数将待压缩数值分解为两个因子。

对于待压缩数值X，Golomb和Rice算法采用如下因子分解方式：

	因子1 = (X-1) / b;
	因子2 = (X-1) mod b;

因子1采用一元编码方式，因子2采用二进制编码方式。

如何制定b的值是Golomb和Rice算法的关键。假设一个待压缩数值序列的平均值为Avg，则Golomb算法设定为:

	b = 0.69 * Avg

这里的0.69是个经验参数，而Rice算法则规定b一定为2的整数次幂，同时b必须是所有小于平均值Avg的2的整数次幂的数值中最接近Avg的数值。例如Avg为113，则b为64，因为如果是b为128则超过了113，如果是32，则不是小于113且最接近113的数，只有64满足条件。

对于Rice算法来说，之所以要规定b为2的整数次幂是为了在实现阶段能够采用掩码操作或比特位移操作等快速运算方法，所以Rice在运算效率方面要优于Golomb。

#### 4.2.5 变长字节算法 (Variable Byte)

前面介绍的压缩算法都是变长比特算法，变成字节算法就是以字节为单位。每个字节第一位表示后一个字节是否也属于当前压缩数值。例如给定数值33549:

	+-+-------+-+-------+-+-------+
	|1|0000010|1|0000110|0|0001101|
	+-+-------+-+-------+-+-------+
	33549 = 2*128*128 + 6*128 + 13

因为每字节第一位为确定后一字节是否为当前数值的压缩字节，1则为是，0则为否。每个字节剩下7个bit，可以记录128个数字。

#### 4.2.6 SimpleX 系列算法

SimpleX系列算法中最常见的是Simple9算法。Simple9最常见的是利用32个bit来作为压缩单位，每个32bit被划分成两个部分。前4个bit作为管理数据存储区，剩下28个bit作为压缩数据存储区，压缩数据存储区根据实际使用情况，可以划分成9种布局类型。
	
			|<- 4 ->|<---------------------- 28 bit ----------------------->|

	B=1		+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
			+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

	B=2		+-+-+-+-+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
			+-+-+-+-+---+---+---+---+---+---+---+---+---+---+---+---+---+---+

	B=3		+-+-+-+-+-----+-----+-----+-----+-----+-----+-----+-----+-----+-+
			+-+-+-+-+-----+-----+-----+-----+-----+-----+-----+-----+-----+X+

	B=4		+-+-+-+-+-------+-------+-------+-------+-------+-------+-------+
			+-+-+-+-+-------+-------+-------+-------+-------+-------+-------+

	B=5		+-+-+-+-+---------+---------+---------+---------+---------+-+-+-+
			+-+-+-+-+---------+---------+---------+---------+---------+X+X+X+

	B=7		+-+-+-+-+-------------+-------------+-------------+-------------+
			+-+-+-+-+-------------+-------------+-------------+-------------+

	B=9		+-+-+-+-+-----------------+-----------------+-----------------+-+
			+-+-+-+-+-----------------+-----------------+-----------------+X+

	B=14	+-+-+-+-+---------------------------+---------------------------+
			+-+-+-+-+---------------------------+---------------------------+

	B=28	+-+-+-+-+-------------------------------------------------------+
			+-+-+-+-+-------------------------------------------------------+

当检查接下来需要存储的28个数都在[0,1]之间的整数，那么可以选择用模式B=1存储；当其中有数不符合，那么检测前14个数是不是都在[0,3]，如果是则可以选择用模式B=2存储，如此类推。

Simple9在某些模式下会有比特浪费的情况，例如B=3、5、9。Simple16的思路与Simple9一样，只是将模式增加到16种，此外Simple16还将浪费的比特位利用起来，例如B=5能存储4个5比特加2个4比特。

#### 4.2.7 PForDelta算法

PForDelta压缩算法是目前解压速度最快的一种倒排文件压缩算法，其出发点是尽可能一次性压缩和解压缩多个数值，尽量SimpleX也是利用这一点，但是PForDelta算法更优。

一次性压缩多个数值面临的困难是: 连续的数值序列有大有小，如果按照最大的数值来决定比特位，则对于笑数值会出现浪费空间情况，如果比特位不够，则大数又无法存储。

PForDelta算法通过找出连续K个数值(一般取K=128)其中10%的大数，根据剩下90%的数值范围确定比特位宽度，而10%采用单独存储。

压缩后的数据分成3个部分: 异常数据存储区、常规数据存储区和异常链表头。

	(K=10, 30% big num)
	待压缩数据块:
						24	40	9	13	31	67	19	44	22	10
						|	==	|	|	|	==	|	==	|	|
						V       V	V	V		V		V	V
	压缩数据:
					1	24	3	9	13	31	1	19	2	22	10	44	67	40	
					--		--				--		--			==	==	==
	
对于上面这个例子，70%的数字小于等于31，可以使用5个比特位的宽度去存储这些数字，30%的大数我们不进行压缩，直接用4个字节来存储，并倒叙放在所有数据后面。

异常链表头存在一个指针，这个指针指向异常链表的第一个数值的位置，上例中为1，表示跳过后续一个常委数值即可获得第1个异常大数的位置，通过链表就可以找到所有大数的位置。

多项实验表明，PForDelta的解压速度是现有压缩算法中最快的。而且性能大幅度超过其他算法。主要原因是其能一次性解压多个数据，尽管恢复异常数据速度比较慢，但是因为其所占比例小，所以影响不大。另一方面PForDelta在解压过程中使用两次遍历，而非一次遍历，是为了避免程序中的IF-ELSE分支，大量分支会影响解压执行速度。

-----------------------------

### 4.3 文档编号重排序 (DocId Reordering)

对于搜索引擎来说，每个网页都被赋予唯一的文档编号(文档ID)，在搜索引擎内部，常见的做法是随机分配一个编号，比如按照爬虫爬取的时间编号。

在索引中某个单词的倒排索引一般会记录包含这个单词的页面的ID，为了压缩索引，倒排索引中的文档ID通常采用D-Gap(文档差值编号)记录。

文档编号重排序就是在文档编号上做文章，希望一个单词的D-Gap尽可能小。为了达到这个目的，我们通常对所有页面进行聚类，使得同一个内容的文章编号相近，减少D-Gap的值。但是由于互联网的页面数目非常之大，文本聚类的速度难以满足实际需求，一个非常有效的启发式规则就是：将页面URL相似的页面编码在一起。因为同一个网站的主题内容是相似的，先通过这种方式将文本聚类，之后再对聚类结果进行重新编号。

------------------------------

### 4.4 静态索引裁剪 (Static Index Pruning)

之间的压缩算法都是无损压缩，但是对于一部分不重要的信息我们实际上可以丢弃以达到更好的数据压缩效果。

对于用户查询来说，一个查询只需要返回相关性最高的K个网页，所以对于一些对用户查询相关性贡献不多的网页，我们可以清除掉。在此思路上有两种算法:一种是以单词为中心的索引裁剪，一种是以文档为中心的索引裁剪。

### 4.4.1 以单词为中心的索引裁剪

假设我们已经有了一个相关性计算函数g(term, doc)，这个函数可以计算倒排索引中某个单词与其对应倒排列表某个文档的相似性，那么可以利用这个函数的计算结果决定是否保留某个索引项。

例如我们定个全局阈值，当得分超过这个阈值才得意保留，否者对应索引项被清除。为了不让某些之后很少索引项的索引被全部清除掉，我们保留至少K个索引项。

### 4.4.2 以文档为中心的索引裁剪

首先判断文档所包含单词的重要性，经过一定的重要性得分计算，保留重要单词，抛弃不重要的单词。对每个文档先进行缩水，然后再在此基础上建立倒排索引。但是这样子对于停用词(eg:的)，这种无实义的词语，他们的倒排索引会是空的。由于以单词为中心的索引裁剪能保重所有词不至于是空的，所以是比较好的索引裁剪方法。

------------------------------

### 第五章 检索模型与搜索排序

### 5.1 布尔模型 (Boolean Model)

如果文档包含某个单词，则为1，否则为0。文档和单词组成二维矩阵。

其缺点是如果包含某个单词，则该文档被判断为相关的；否则判断为不相关的。输出结果是二元的，无法对搜索结构进行相关性排序。

				|	Doc1	|	Doc2	|	Doc3	|	Doc4	|	Doc5	
	------------+-----------+-----------+-----------+-----------+-----------
		word1	|			|	  √		|	  √		|			|	  √
	------------+-----------+-----------+-----------+-----------+-----------
		word2	|	  √		|			|	  √		|	  √		|	  √
	------------+-----------+-----------+-----------+-----------+-----------
		word3	|			|			|	  √		|	  √		|

### 5.2 向量空间模型 (Vector Space Model)

#### 5.2.1 文档表示

把每个文档看作t维特征组成的一个向量，特征可以采用不同方式，可以是单词、词组、N-gram片段等多种形式，最常见的还是以单词为特征。每个特征回根据一定依据计算其权重，这t维带权重特征共同构成一个文档。

用户的查询耶被视作一个特殊的文档。

				| feature1	| feature2	| feature3  
	------------+-----------+-----------+-----------
		Doc1	|	w11		|	w12		|	w13
	------------+-----------+-----------+-----------
		Doc2	|	w21		|	w22		|	w23
	------------+-----------+-----------+-----------
		Doc3	|	w31		|	w32		|	w33
	------------+-----------+-----------+-----------
		Doc4	|	w41		|	w42		|	w43
	------------+-----------+-----------+-----------
		query	|	q1		|	q2		|	q3

#### 5.2.2 相似性计算

Cosine相似性定义如下:

	Cosine(Q, Di) = (Sum_j{wij*qj}) / (sqrt(Sum_j{wij^2} * Sum_j{qj^2}))

分子是query与document的点乘，分母是规则化，因为长文档通常特征权值比短文档大，所以需要规则化。

Cosine共识中规范化方法的一个明显缺陷是：存在对长文档的过分抑制。如果同时有长文档和段文档包含同样的主题内容，由于长文档还包含其他内容，使得规范化后相似性较小，所以即使长文档包含短文档所有内容，其相似度数值也比较小。

#### 5.2.3 特征权重计算

文档和查询转换为特征向量时，每个特征都会有一定的权值，特征权值的计算框架一般是TF-IDF。词频TF和逆文档频率IDF。

**词频因子 (TF)**

即一个单词在文档中出现的次数。一个反复出现的词，往往能表达文档的主题信息。基于不同出发点，可以采用不同的公式计算。最直接就是利用词频数，例如某单词在文档出现5词，则TF=5。一种词频因子的变体公式是:

	Wtf = 1 + log(TF)

前面的1是为了平滑计算用的，因为当TF=1，不加1的Wtf=0，会让人误以为这个单词没有出现过。Log是为了抑制过大的差异。

另一种单词词频因子的变体公式是:

	Wtf = a + (1-a)*(TF/Max((TF))

这种方法称为增强型规范化TF，其中a是调节因子，经验值取0.4。这样做的原因是对长文档的抑制。因为长文档与短文档相比，长文档的TF值会普遍更高，但这并不代表长文档与查询更相关。除以单词词频除以最大词频可以消除长度因素的影响。

**逆文档频率因子 (IDF)**

其计算公式为:

	IDFk = log(N/nk)

其中N代表文档集合的总数，nk代表特征单词k在其中多少个文档出现过，即文档频率。当所有文档都出现某个词的时候，这个词的IDF最小，即这个词区分不同文档的能力越差；当只有一个文档出现这个词的时候，这个词的IDF最大，即这个词带有的信息量越多。

**TF-IDF 框架**

一般是将两者相乘作为特征权值，权值越大，则越可能是好的指示词:

	Weight_word = TF * IDF

经过几十年的不断探索，向量空间模型已经相当成熟，被广泛使用。

--------------------

### 5.3 概率检索模型

#### 5.3.1 概率排序原理

用户发出一个查询请求，如果我们把文档集合划分成两个类别，相关文档子集和不相关文档子集，于是可以将相关性转换呈一个分类问题。

P(R|D)表示给定一个文档与查询相关的概率；P(NR|D)表示给定一个文档与查询不相关的概率。根据贝叶斯规则对两个概率值进行改写:

	P(R|D) = P(D|R)*P(R) / P(D)
	P(NR|D) = P(D|NR)*P(NR) / P(D)

相关性计算的目的是要判断是否P(R|D) > P(NR|D)，将公式改写成下面形式:

	P(R|D) > P(NR|D) == P(D|R)/P(D|NR) > P(NR)/P(R)

尽管概率模型将相似性判断转换成一个二值分类问题，但是搜索并不需要真正的分类，只需要将搜索结果按照相似性由高到低排序，所以只需要将文档按照 P(D|R)/P(D|NR)大小排序即可。于是问题进一步转化成估算因子P(D|R)和P(D|NR)，而二元独立模型提供了计算这些因子的框架。

#### 5.3.2 二元独立模型 (Binary Independent Model)

二元独立模型((BIM)做出了两个假设:

**假设1: 二元假设**

一篇文档在由特征进行表示的时候，以特征“出现”和“不出现”两种情况表示，不考虑词频等其他因素。这是为了简化复杂问题。

**假设2: 词汇独立性假设**

假设文档中出现的单词之间没有任何关系。虽然这个假设与事实不符，但是为了简化计算，很多方法都会做这样的独立性假设。

现在对于一个文档D表示为{1,0,1,0,1}，表示这个文档包含第1、3、5个单词不包含第2、4个单词。我们用pi表示第i个单词在相关文档集合中出现的概率，用si表示第i个单词在不相关文档集合中出现的概率:

	P(D|R) = p1 * (1-p2) * p3 * (1-p4) * p5
	P(D|NR) = s1 * (1-s2) * s3 * (1-s4) * s5

	P(D|R)/P(D|NR)	= Product_di=1{pi/si} * Product_di=0{(1-pi)/(1-si)}
					= Product_di=1{(pi(1-si))/(si(1-pi))} * Product{(1-pi)/(1-si)}

由于Product{(1-pi)/(1-si)}对所有单词一样，所以可以去掉。

	P(D|R)/P(D|NR) = Product_di=1{(pi(1-si))/(si(1-pi))}

出于计算方便，加上log，得到相关性估值公式:

	Sum(i:di=1) log((pi(1-si)) / (si(1-pi)))

下面是如何计算概率pi和si的表格:

	--------+-------------------+-----------------------+---------
			|	Relative Doc	|	Non-Relative Doc	|	Doc
	--------+-------------------+-----------------------+---------
	 di = 1	|		ri			|		ni-ri			|	ni
	 di = 0	|		R-ri		|		(N-R)-(ni-ri)	|	N-ni
	 Doc	|		R			|		N-R				|	N
	--------+-------------------+-----------------------+---------

其中N表示文档集合总数，R表示相关文档总数，对于某个单词di，ni表示包含这个单词的文档个数，ri表示其中相关的文档个数。

	pi = (ri + 0.5) / (R + 1.0)
	si = (ni - ri + 0.5) / (N - R + 1.0)

实验表明二元独立模型计算相关性效果并不好，但是这个模型是非常成功的概率模型BM25的基础。

### 5.3.3 BM25模型

---
layout: post
title:  "《这就是搜索引擎 核心技术详解》读书笔记"
date:   2015-03-24 12:00:00
tags:	tech
myTag:	algorithm

---

# 《这就是搜索引擎 核心技术详解》读书笔记

---------------------------------------------------

### 第二章 网络爬虫

### 2.4 抓取策略

在爬虫系统中，待抓取URL队列是很关键的部分，需要爬虫爬取的网页URL在其中顺序排列，形成一个队列结构，调度程序每次从队列头取出某个URL，发送给网页下载器下载页面内容，每个新下载的页面包含的URL会追加到待抓取URL队列的末尾，形成循环，整个爬虫系统由这个队列驱动运转的。

爬虫的不同爬取策略，是利用不同的方式确定待抓取URL队列中URL的优先顺序。抓取策略中，比较有代表性的解决方案有4种: 宽度优先便利策略、非完全PageRank策略、OCIP策略以及大站优先策略。

### 2.4.1 宽度优先遍历策略 (Breath First)

宽度优先遍历策略是一种简单直观而且历史悠久的遍历方法，在搜索引擎爬虫一出现就开始采用，新提出的爬取策略往往会将这种方法作为比较基准。值得注意的是，很多新的方法实际效果不见得比宽度优先遍历策略好，所以至今这种方法也是很多实际爬虫系统优先采用的爬取策略。

宽度优先策略就是`"将新下载网页包含的链接直接追加到待抓取的URL队列末尾"`。

实验表明这种策略效果很好，虽然看似机械，但实际上的网页抓取顺序基本是按照网页的重要性排序的。之所以如此，有研究人员认为：如果某个网页包含很多入链，那么更有可能被宽度优先遍历策略早早抓到，而入链个数从侧面体现了网页的重要性，即实际上宽度优先遍历策略隐含了一些网页优先级假设。

### 2.4.2 非完全PageRank策略 (Partial PageRank)

PageRank 可以用来衡量网页的重要性，可以想到用PageRank的思想来对URL优先级进行排序。但是PageRank是一个全局性算法，也就是当所有网页都下载完成后，其计算结果才是可靠的，而爬虫在爬取页面的过程中只能看到一部分页面，所以在抓取阶段的页面是无法得到可靠PageRank得分的。

但是我们可以在这个不完整的网页子集内计算PageRank。这就是非完全PageRank策略的基本思路: `对已下载的网页，加上待抓取URL队列中的URL一起，形成网页集合，在此集合内进行PageRank计算，计算完成后，将待抓取URL队列里的页面按照PageRank得分由高到低排序，形成的序列就是爬虫接下来应该依次爬取的URL列表。`这就是"非完全PageRank"策略。

如果每抓取一个页面就计算一次非完全PageRank值，明显效率太低，在现实中不可行。一个折中的办法是: 每当新下载K个页面后，再进行非完全PageRank的计算。但是这样又引来一个新的问题: 在展开下一轮PageRank计算之前，从新下载的页面抽取出包含的链接，很有可能这些链接的重要性非常高，理应优先下载。`非完全PageRank赋予这些新抽取出来但是又没有PageRank值的网页一个临时PageRank值，将这个网页的所有入链传导的PageRank值汇总，作为临时的PageRank值，如果这个值比待抓取URL队列中以及计算出来的PageRank值的网页高，那么优先下载这个URL`。

非完全PageRank看上去相对复杂，不同的实验结果有些表明非完全PageRank结果略优，有些结果则恰恰相反。更有研究人员指出: 非完全PageRank计算得出的重要性和完整的PageRank计算结果差异很大，不应作为衡量抓取过程中URL重要性的依据。

### 2.4.3 OCIP 策略 (Online Page Importance Computation)

OCIP可以看成是一种改进的PageRank算法。`在算法开始前，每个页面都给予相同的"现金"(cash)，每当下载某个页面P后，P将自己拥有的现金平均分配给页面中包含的链接页面，把自己的现金清空。而对于待抓取的URL队列中的页面，则根据其手头上拥有的现金金额多少排序，优先下载现金最充裕的页面。`OICP从大的框架和PageRank思路基本一直，区别在于: PageRank每次需要迭代计算，而OCIP策略不需要迭代过程，所以计算速度远远快语PageRank，适合实时计算使用。同时，PageRank在计算时，存在向无链接关系页面远程跳转的过程，而OCIP没有这一计算因子。实现结果证明，OCIP是中较好的重要性衡量策略，效果略优于宽度优先遍历策略。

### 2.4.4 大站优先策略 (Larger Sites First)

大站优先策略思路很直接: `以网站为单位衡量网页重要性，对于待抓取URL队列中的页面，根据所属网站归类，如果那个网站等待下载的页面最多，则优先下载这些链接。`其本质思想倾向于优先下载大型网站，因为大型网站往往包含更多的页面，鉴于大型网站往往是著名企业的内容，其网页质量一般较高，所以这个思路简单，但有一定依据。实现表明这个算法效果也要略优于宽度优先遍历策略。

---------------------------------------------

### 第三章 搜索引擎索引

### 3.1 索引基础

### 3.1.1 单词-文档矩阵

单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型。

	文档.	|	doc1	|	doc2	|	doc3	|	doc4	|	doc5
	--------------------------------------------------------------------
	词汇1	|	√		|			|			|			|	√
	词汇2	|			|	√		|	√		|			|	
	词汇3	|			|			|			|	√		|	
	词汇4	|	√		|			|			|			|	√
	词汇5	|			|	√		|			|			|	
	词汇6	|			|			|	√		|			|	

搜索引擎的索引就是实现单词-文档矩阵的数据结构。有很多不同方式实现上述概念模型，比如倒排索引、签名文件、后缀树等。各项实验数据表明，倒排索引是单词-文档映射关系的最佳实现方式。

### 3.1.3 倒排索引简单实例

中文和英文不同，单词之间没有明确的分隔符号，所以首先要用分词系统将文档自动切分成单词序列，这样每个文档就转化为由单词序列构成的数据流。每个不同的单词赋予唯一的`单词编号`，同时记录下哪些文档包含这个单词。

在单词对应的倒排列表中不仅记录了文档编号，还记载了单词频率信息(TF)，即单词在某个文档中出现的次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相识度是一个很重要的计算因子，所以将其记录在倒排列表中，方便后续排序进行分值计算。

除了记录文档编号和单词频率信息外，实用的倒排索引还额外记载了两类信息，即每个单词对应的文档频率信息以及单词在某个文档出现的位置信息。文档频率信息代表了在文档集合中由多少个文档包含某个单词，这个信息在搜索结构排序计算中也是一个重要的因子。而单词在某个文档中出现位置的信息并非索引系统一定要记录的，之所以说这个信息对搜索系统来说并非必要，是因为位置信息只有在支持短语查询的时候才能够派上用场。


### 3.2 单词词典

单词词典是用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里面查询，能够获得相应的倒排列表。对于一个规模很大的文档集合来说，可能包含百万个不同单词，能够快速定位某个单词，直接影响搜索速度，所有需要高效的数据结构对单词词典进行构造和查找，常用的数据结构包括哈希加链表结构和树形词典结构。

### 3.2.1 哈希加链表

主体是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在通途链表里，相同哈希值的单词行程链表结构。

在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档时，对于某个文档中出现的单词T，首先利用哈希函数获得哈希值，之后根据哈希值读取对应哈希表项其中保存的指针，找到对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解释的文档里已经出现过。如果冲突链表里没有这个单词，说明该单词是首次出现，则加入冲突链表。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就简历起来了。

### 3.2.2 树形结构

B树(或者B+树)是另一种高效查找结构。B树与哈希查找方式不同，需要字典能够按照大小排序，而哈希方式无需数据满足此项要求。

B树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到了根据词典项比较大小进行导航的作用。最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。

-----------------------------------

### 3.3 倒排列表(Posting List)

	+-------+	+---------------------------+---------------------------+---------------------------+
	| word	|-->| [DocID,TF,<pos1,pos2,...>]| [DocID,TF,<pos1,pos2,...>]| [DocID,TF,<pos1,pos2,...>]|
	+-------+	+---------------------------+---------------------------+---------------------------+
						Posting Item				Posting Item				Posting Item
							|															|			
							+-----------------------------+-----------------------------+
													Posting List
	Dictionary Item

上图是之前介绍的简单索引例子。在实际的搜索引擎系统中，并不存储倒排索引项中的实际文档编号，而是取而代以文档编号差值(D-Gap)。文档编号差值是倒排列表中相邻的两个倒排索引项文档编号的差值，一般在索引构建过程中，可以保证倒排列中后面出现的文档编号大于之前出现的文档编号，所以文档编号差值总是大于0的整数。

之所以要对文档编号进行差值计算，主要原因是为了更好地对数据进行压缩，原始文档编号一般都是大数值，通过差值计算，就有效地将大数值转换为了小数值，而有助于增加数据地压缩率。第四章将详细讲述索引压缩及其原因。

------------------------------------

### 3.4 建立索引

### 3.4.1 两遍文档遍历法 (2-Pass In-Memory Inversion)

#### 第一次文档遍历

第一次扫描文档集合时，该方法并没有立即开始建立索引，而是收集全局统计信息。比如文档集合包含的文档个数N，文档集合内所包含的不同单词个数M，每个单词在多少个文档中出现过的信息DF。将所有单词对应的DF值全部相加，就可以知道建立最终索引所需的内存大小是多少，因为一个单词对应的DF值如果是10，说明有10个文档包含这个单词，那么这个单词对应的倒排列表应该包含10项内容，每一项计算某个文档的文档ID和单词在该文档对应的出现次数TF。

在获得上述3类信息后，就可以知道最终索引的大小，于是在内存中分配足够大的空间，用来存储倒排索引内容。在内存中可以开辟连续存储区域，因为第一遍扫描已经获得每个单词的DF信息，所以将连续存储区划分成不同的片段，词典内某个单词根据自己对应的DF信息，可以通过指针，指向属于自己的内存片段的起始位置和终止位置，将来在第二次扫描时，这个单词对应的倒排列表信息就会被填充进这个片段中。

综上所述，第一遍扫描的主要目的是获得一些统计信息，根据统计信息分配内存资源，建立好单词对应倒排列表在内存中的位置信息。

#### 第二遍文档遍历

第二次扫描的时候，开始真正建立每个单词的倒排列表信息，对每个单词而言，获得包含这个单词的每个文档的文档ID，以及这个单词在文档中的出现次数TF，这样开始不断填充第一遍扫描所分配所指向的内存空间。当第二遍扫描结束的时候，分配的内存空间正好被填充满，而每个单词用指针指向的内存区域其起始位置和终止位置之间的数据就是这个单词对应的倒排列表。

所以的构建完全是在内存中完成的，这就要求内存一定要大，否则如果文档集合太大，内存未必能够满足需求。在建立索引的过程中，从磁盘读取并解析文档基本是最耗时的步骤，而两遍文档遍历法需要进行两遍遍历，所以速度上不占优，在实际中采用这种方法的系统并不常见。

### 3.4.2 排序法 (Sort-based Inversion)

#### 中间结果内存排序

两次遍历法对内存消耗要求高，当文档集合非常大时，可能因为内存不够，导致无法建立索引。排序法对次做了改进，该方法在建立索引的过程中，始终在内存中分配固定大小的空间，用来存放词典信息和索引的中间结果，当分配空间被消耗光的时候，把中间结果写入磁盘，清空内存中间结果所占空间，以用做下一轮存放索引中间结果的存储区。

读入文档后，对文档进行编号，赋予唯一的文档ID，并对文档进行内容解析。对于文档中出现的单词，通过查词典将单词转换为对应的单词ID，如果词典中没有这个单词，则赋予单词唯一的单词ID并插入词典。在完成类由单词映射为单词ID的过程之后，可以对该文档内每个单词建立一个(单词ID，文档ID，单词频率)三元组，这个三元组就是单词对应文档的倒排列表项，将这个三元组追加进中间结构存储区末尾。

随着新文档不断被完成，存储三元组的中间结果所占内存会越来越大，词典所包含新单词也越来越多，当分配的内存定额被占满时，对三元组中间结构进行排序。排序原则是: 主键是单词ID，首先按单词ID由小到大排序；次键是文档ID，即相同单词ID的情况下，按照文档ID从小到大排序。通过这个方法，三元组变成有序形式。为了腾出内存空间，将排好序的三元组写入磁盘临时文件中。需要注意的是: `在建立索引的过程中，词典是一直存储在内存中的，每次清空的内存只是将中间结果写入磁盘。`随着处理文档的增加，词典占用的内存会增多，由于分配内存是固定大小，而词典占用内存越来越大，即越往后，可以用来存储三元组的空间越来越少。

#### 合并中间结果

因为每一轮处理都会在磁盘上产生一个对应的中间结果文件，在合并中间结果的过程中，系统为每个中间结果文件在内存中开辟一个数据缓冲区，用来存放文件的部分数据。因为在形成中间结果文件前，已经按照单词ID和文档ID进行了排序，所以进入缓冲区的数据已经是有序的。合并过程中，将不同缓冲区中包含同一个单词ID的三元组进行合并，如果某个单词ID的所有三元组全部合并完成，说明这个单词的倒排列表已经构建完成，则将其写入最终索引中，同时将各个缓冲区中对应的单词ID的三元组清空，当所有中间结果文件都依次被读入缓冲区，合并完成后，就形成了最终的索引文件。

### 3.4.3 归并法 (Merge-based Inversion)

排序法只是将中间结果写入磁盘，而词典信息一直在内存中进行维护，随着处理文档越来越多，词典里包含的词典项占用越来越多内存，导致后期中间结果可用内存越来越少。归并法对此做了改进，即每次将内存中数据写入磁盘时，包括词典在内的所有中间信息都被写入磁盘，这样内存所有内容就可以被清空，后续建立索引可以使用全部的定额内存。

排序法在内存中存放的是词典信息和三元组数据，在建立过程中，词典和三元组数据并没有直接的联系，词典只是为了将单词映射为单词ID。而在归并法中则是在内存中建立一个完整的内存索引结构，相当于对目前处理的文档子集单独在内存中建立起一整套倒排索引，和最终索引相比，其结构和形式是相同的，区别只是这个索引只是部分文档的索引而非全部文档的索引。










